# Attention

## Background

某些机器学习任务的输入是大小（长度）可变的向量集合，例如：

- 文本
- 音频
- 网络（关系图、分子等）

随着任务的不同，输出也有不同的类型，例如：

- 每个向量有一个相应的标签（词性标注）
- 所有向量有一个标签（情感分析）
- 标签的个数不确定（翻译）

<blockquote alt="info">
    <p>
        输出标签数量不确定的任务统称为 Seq2Seq（Sequence-to-Sequence）的任务
    </p>
</blockquote>

对于这种任务来说，从直觉上讲，模型应当有考虑上下文或全局的功能，那么采取什么样的方法或架构可以让模型拥有这种能力呢？

## Self-attention

### Progress

在自注意力机制中，输出序列中的向量个数和输入序列中的向量个数是相同的，之于输入序列（向量集合）中的每个向量 $a_{i}$ 来说，都有各自相应的 3 个属性

1. 查询 $q_{i}$
2. 键 $k_{i}$
3. 值 $v_{i}$

<blockquote alt="warn">
    <p>
        每个属性都由向量和相应的可学习矩阵相乘得到，可学习矩阵之于不同的向量来说是共享的
    </p>
</blockquote>

利用每个向量的查询点乘其它向量（包括自己）的键得到每个向量和所有向量的一系列相关性系数，再将每个向量的相关性系数以某种方式标准化，之后将每个向量标准化之后的相关性系数分别和各自相应的值相乘再求和就可以得到每个向量考虑了全局输入之后的的输出

<blockquote alt="success">
    <p>
        输出在经过全连接网络之后就是最终的输出了，也可以作为下一个自注意力块的输入
    </p>
</blockquote>

### Matrix Representation

我们假设输入序列可以用矩阵 $I$ 来表示，而 3 个用来计算每个向量属性的可学习矩阵分别用 $W^{q}$, $W^{k}$ 和 $W^{v}$ 来表示，那么序列中每个向量的属性可以表示如下
$$
Q = W^{q} * I \\
K = W^{k} * I \\
V = W^{v} * I \\
$$
利用 $Q$ 和 $V$ 来计算相关性系数
$$
A = K^{T} * Q
$$
再通过 softmax 得到注意力矩阵 $A'$ 之后，和 $V$ 相乘就是最终的输出 $O$
$$
O = V * A'
$$

### Multi-head Attention

在某些时候，输入序列中的相关性或许体现在不同的方面，此时唯一的一组查询、键和值或许不可以很好地表征向量之间的关系，所以之于每个序列，我们可以同时有多个相互独立的查询、键和值，也就是说有多个相互独立的可学习矩阵组，每个矩阵组构造一组查询、键和值

<blockquote alt="danger">
    <p>
        必须注意的是，不同组之间的计算是相互独立的
    </p>
</blockquote>

如此以来，每个向量将同时拥有多个相关的输出结果，此时我们可将它们连接起来得到一个较长的向量，之后再和某个转化矩阵相乘得到最终的结果

<blockquote alt="warn">
    <p>
        在最后实现转换功能的矩阵应该也是可学习的
    </p>
</blockquote>

### Positional Encoding

虽然自注意力机制可以考虑到所有的输入序列，不过似乎未包含序列次序的信息，为了让自注意力机制在学习的同时考虑到次序的信息，我们可以在序列中的每个向量中加上一个可以表示特定位置的特殊向量

<blockquote alt="info">
    <p>
        特殊的位置向量可以手工制作也可以由某个网络生成
    </p>
</blockquote>

### Applications

自注意力机制可以用在语音或者图像上，之于图像来说，表示方法一般是包含 3 个维度的张量，我们实际上也可以将它视作一系列向量：每个像素的 RGB 值构成一个向量，那么图像的大小就是序列的大小了

我们知道传统的 CNN 是专门用于图像的网络架构，实际上，CNN 是自注意力机制的一个特殊的子集而已，具体来说，CNN 中的卷积核所考虑的范围往往是规定大小的方块，所以相关性总是局限在特定的范围之内，而之于考虑所有向量自注意力机制来说，考虑哪些范围是由模型自己学习到的，而且不存在范围的限制

<blockquote alt="success">
	<p>
        所以说，自注意力机制所能够表示的函数集合是大于 CNN 的，也就意味着自注意力机制往往应当有很大的数据集才可以有很好的效果，不然容易过拟合
    </p>
</blockquote>

<blockquote alt="info">
    <p>
        之于专门用于序列的 RNN 网络来说，自注意力机制的平行计算是不可比的
    </p>
</blockquote>

## Transformer

Transformer 是一种基于 Encoder 和 Decoder 的架构，在 Encoder 和 Decoder 中都采用了自注意力机制

### Encoder

Transformer 的 Encoder 由若干个基于自注意力机制的块构成，块中的基本结构由多头注意力机制和全连接网络构成，不过在每个部分中都有残差的思想，具体来说

1. 将多头注意力机制的输入加在原本输出上作为输出
2. 将全连接网络的输输入加载原本输出上作为整个注意力块的输出

<blockquote alt="info">
    <p>
        在残差连接之后，存在一个 Layer Normalization
    </p>
</blockquote>

### Decoder

有 2 种不同类型的 Decoder，分别是基于自回归的（Autoregressive，AT）和不基于自回归的（Non-autoregressive，NAT），它们从某种程度上来说是截然相反的，各自有不同的优劣势

![transformer-graph](C:\Users\vcc\Documents\Classic_Papers\Attention\transformer-graph.png)

#### Autoregressive

基于自回归的模型中，结果序列中的向量是依次输出的，也就是说，Decoder 首先以一个表示开始的特殊标记作为输入，输出一个结果序列中的向量，然后再将此向量作为 Decoder 的输入以生成下一个结果序列中的向量，以此类推，在生成过程中，必须由网络自己决定什么时候停止生成下一个向量，此时，网络将一个表示结束的特殊标记作为输出

从 Decoder 的模型架构来看，大体上和 Encoder 相差不多，在每个块中除了普通的多头注意力机制和去全连接网络之外，在最开始有一个 Mask 的多头注意力机制，在此类型的注意力机制中，相关性系数等计算不再考虑向量右侧的所有向量，而仅考虑所有左侧的向量，原因在于 Decoder 以自己的输出作为下一次模型的输入，无法预知序列右侧的内容

<blockquote alt="info">
    <p>
        Mask 的多头注意力机制任然有残差和 Layer Normalization
    </p>
</blockquote>

#### Non-autoregressive

在非自回归的方式中，结果序列不再是由 Decoder 依次输出的向量组合而成，而是一次性输出所有向量，与之相对应的，基于 NAT 的 Decoder 的输入也不再是唯一的向量，而是规定个数的表示开始的特殊向量

和 AT 的 Decoder 相比，由以下优点：

1. 平行计算（一次性出结果）
2. 稳定的生成（输入序列的长度确定）

不过也存在很明显的缺点，那就是效果不如 AT 的 Decoder

### Connection

Decoder 和 Encoder 之间的差异还有一个地方，那就是 Cross Attention，在每个块中的多头注意力机制部分，它的输入除了来自于 Mask 的多头注意力机制部分之外，还有来自于 Encoder 的输入，在此部分中，来自 Mask 注意力机制部分的输出结果和 Encoder 的结果计算相关性参数

<blockquote alt="success">
    <p>
        在 Encoder 和 Decoder 之间有很多种连接的方法
    </p>
</blockquote>